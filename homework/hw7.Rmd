---
title: "M/STAT 501 Fall 2022: Homework 7"
subtitle: Due _Monday_, Dec 5 by 5:00pm in Gradescope
output: 
   pdf_document:
      includes:
            in_header: preamble.tex
urlcolor: blue
---

<!-- Material covered: 4.5, 4.3, 4.6, extra random vector stuff: covariance and correlation, bivariate transformations, multivariate distributions -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Directions**: For this homework, there are three different types of problems: 1) _Graded for Accuracy Problems_, 2) _Graded for Completion Problems_, and 3) _Extra Practice Problems_. Please turn in your work for the _Graded for Accuracy Problems_ and the _Graded for Completion Problems_, as well as any _Sources Used_ by the due date via Gradescope. Show work and/or provide justification for credit---neatness counts! _The Extra Practice Problems_ do not need to be turned in, and the solutions will be posted for you to check and compare your work.

You are encouraged to use R Markdown for your homework, but it is not required. However, **you must use R Markdown for any problems that require the use of R; include both your R code and the R output on the rendered pdf**.

## Graded for Accuracy Problems (turn in)

1. Suppose $X$ and $Y$ are independent Exponential random variables, both with mean $\theta$. Define $U = X-Y$.
    a. Use the Jacobian method for transformations of continuous random variables (Equation (4.3.2) on p. 158) to find the marginal pdf of $U$.
    b. Verify that you get the same answer if you find the moment generating function of $U$ and use it to determine the distribution of $U$.  
    
2. Let $\mathbf{y}_i \sim Multinoulli(\mathbf{p})$, $i = 1, \cdots, n$, where $\sum_{j = 1}^K p_j = 1$.  The Multinoulli pmf is given as:
    \[
    f(\mathbf{y}) = \begin{cases}\begin{array}{ll}
     \prod_{j = 1}^K p_j^{y_j} & \mathbf{y} \in \left\{\mathbf{v} \in \{0,1\}^K : \sum_{i = 1}^K v_i = 1\right\}\\
    0 & \text{else}
    \end{array}\end{cases}
    \]
and let $\mathbf{x} = \mathbf{y}_1 + \cdots + \mathbf{y}_n$.
    a. Show that $\mathbf{x} \sim Multinomial(n, \mathbf{p})$.
    b. Find the joint moment generating function of $\mathbf{x}$.  
    
    
3. Let $\mathbf{x}= (X_1, X_2)'$ be bivariate normal with $\bs{\mu}_{X} = \begin{pmatrix} 1 \cr 2\end{pmatrix}$ and $\bs{\Sigma}_X = \begin{pmatrix} 2 & 1 \\1 &2 \end{pmatrix}$, and let  
\begin{align*}
\mathbf{y} | \mathbf{x} \sim N_2\left(\bs{\mu}_{\mathbf{y} | \mathbf{x}} = \begin{pmatrix} x_1 \\ x_1 + x_2\end{pmatrix}, \bs{\Sigma}_{\mathbf{y} | \mathbf{x}} = \begin{pmatrix} 1 & 0 \\0 &1 \end{pmatrix}\right)
\end{align*}
    a. Denoting $\bm{y} = (Y_1, Y_2)'$, what is the distribution of $Y_2 | Y_1 = y_1$? (Note that this is a univariate distribution.)
    b. What is the distribution of $\mathbf{w} = \mathbf{x} - \mathbf{y}$? (Note that this is a bivariate distribution.)  


    
## Graded for Completion Problems (turn in)

1. Let $X_1,\ldots, X_n$ and $Y_1,\ldots, Y_m$ be random variables, all of whose expectations and variances exist. Let $a_1,\ldots, a_n$, $b_1\ldots, b_n$, $c_1\ldots, c_m$, and $d_1,\ldots, d_m$ be constants not all equal to zero.
    a. Show
    $$
    Cov\left(\sum_{i=1}^n (a_iX_i + b_i), \sum_{j=1}^m(c_jY_j + d_j)\right) = \sum_{i=1}^n \sum_{j=1}^m a_ic_j Cov(X_i, Y_j).
    $$
    b. Use the result in part a. to show that
    \begin{eqnarray*}
    Var(\sum_{i=1}^n (a_iX_i + b_i)) &=& \sum_{i=1}^n a_i^2 Var(X_i) + \sum\sum_{i\neq j}a_ia_jCov(X_i, X_j) \\
    &=& \sum_{i=1}^n a_i^2 Var(X_i) + 2\sum\sum_{i< j}a_ia_jCov(X_i, X_j).
    \end{eqnarray*}
    
2. Consider the vector form of a multivariate normal pdf with mean $\boldsymbol{\mu}$ and variance-covariance matrix $\boldsymbol{\Sigma}$:
$$
f(\bm{y}) = \frac{\exp\left\{-\frac{1}{2}(\bm{y} - \bs{\mu})'\bs{\Sigma}^{-1}(\bm{y} - \bs{\mu})\right\}}{|\bs{\Sigma}|^{\frac{1}{2}}(2\pi)^{\frac{n}{2}}}.
$$
Show that, for $n = 2$, this pdf reduces to the form given in Definition 4.5.10 on p.\ 175 of our textbook.

3. Let $Y$ be a positive (univariate) random variable.  (Assume that the expectations below exist).  Show that:
    a. $E[Y^\alpha] \leq (E[Y])^\alpha$ when $0 \leq \alpha \leq 1$
    b. $E[Y^\alpha] \geq (E[Y])^\alpha$ when $\alpha \leq 0$ or $\alpha \geq 1$
    c. $\frac{E[Y^{\alpha-1}]}{E[Y^\alpha]} \leq E[1/Y]$ for $0 < \alpha < 1$


## Sources Used (turn in)

At the end of your assignment, list and cite **all** sources used when working on this assignment, including individuals with whom you discussed the problems, old homework assignments you may have found, discussion boards, websites, etc.
If you did not use any sources besides our textbook or the course notes, please note this.


## Extra Practice Problems (do not turn in, solutions posted in D2L)

1. Let $X$ and $Y$ denote the values of two stocks at the end of a five-year period. $X$ is uniformly distributed on the interval $(0, 12)$. Given $X = x$, $Y$ is uniformly distributed on the interval $(0, x)$. Find the covariance and correlation between $X$ and $Y$.

2. Let $X$ and $Y$ be two random variables with finite means and variances. Under what conditions are $X+Y$ and $X-Y$ uncorrelated?

3. Casella and Berger book problems:  4.45, 4.46, 4.62, 4.64

